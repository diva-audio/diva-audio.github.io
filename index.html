<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DiVA (Distilled Voice Assistant)</title>
  <meta name="description" content="Distilling an End-to-End Voice Assistant from Speech Recognition Data Using Pretrained Models.">

  <!-- Facebook -->
  <meta property="og:url" content="https://value-nlp.github.io/DiVA-Demo">
  <meta property="og:type" content="website">
  <meta property="og:title" content="DiVA (Distilled Voice Assistant)">
  <meta property="og:description" content="Distilling an End-to-End Voice Assistant from Speech Recognition Data Using Pretrained Models.">
  <meta property="og:image" content="https://value-nlp.github.io/DiVA-Demo/static/images/hero.png">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="value-nlp.github.io">
  <meta property="twitter:url" content="https://value-nlp.github.io/DiVA-Demo">
  <meta name="twitter:title" content="DiVA (Distilled Voice Assistant)">
  <meta name="twitter:description" content="Distilling an End-to-End Voice Assistant from Speech Recognition Data Using Pretrained Models.">
  <meta name="twitter:image" content="https://value-nlp.github.io/DiVA-Demo/static/images/hero.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" sizes="192x192" href="images/android-desktop.png">

  <meta name="viewport" content="width=device-width, height=device-height, initial-scale=1">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
  <style>

    .SAmE {
        color: #D55E00; /* myorange */
        font-family: monospace;
        font-weight: bold;
    }
    .SAsE {
        color: #2F5596; /* mydarkblue */
        font-family: monospace;
        font-weight: bold;
    }
    .highlight {
        background-color: #F9CD69;
        font-weight: bold;
    }
    .quote {
      color: #073ea2;
      font-style: italic; 
      font-size: 1.2em;
      max-width: 50%;
      margin: 0 auto; 
      font-weight: bold; 
      margin-bottom: 20px;
      text-align: center;
    }
    table {
      max-width: 60%;
      border-collapse: collapse;
      margin: 0 auto;
      margin-top: 20px;
      text-align: center;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 8px;
      font-size: 14px;
    }
    th {
      background-color: #f2f2f2;
      color: #333;
    }
    tr:nth-child(even) {
      background-color: #f9f9f9;
    }
    td:nth-child(2), td:nth-child(3) {
      text-align: center;
    }
    .dashline {
      border-top: 1px dashed #999;
    }
    caption {
      caption-side: bottom;
      font-size: 0.9em;
      padding-top: 10px;
      color: #555;
    }
    .quote-text {
      color: #073ea2;
      font-weight: bold;
    }
  </style>

<body>
  

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="margin-bottom:0"><b style="color:#820000">Di</b>stilled <b style="color:#820000">V</b>oice <b style="color:#820000">A</b>ssistant</h1>
	        
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/WillHeld/DiVA-llama-3-v0-8b"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Model Link</span>
                </a>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://colab.research.google.com/drive/1Ab3z_BjM_FblAyne7W7hbnT6gLWOhram?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-chart-bar"></i>
                  </span>
                  <span>Evaluation Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/Helw150/levanter/blob/will/distill/src/levanter/models/via.py"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Training Code</span>
                  </a>
            </div>
	  </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://williamheld.com/">Will Held*</a><sup>1,3</sup>,</span>
	    <br/>	    
	    <span class="author-block"><a href="https://yocodeyo.github.io/">Ella Li</a><sup>2</sup>,</span>
	    <span class="author-block"><a href="https://michryan.com/">Michael Ryan</a><sup>3</sup>,</span>
	    <span class="author-block"><a href="https://wyshi.github.io/">Weiyan Shi</a><sup>3</sup>,</span>
            <span class="author-block"><a href="https://stevenyzzhang.github.io/website/">Yanzhe Zhang</a><sup>1,3</sup>,</span>
	    <br/>
            <span class="author-block">
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Georgia Institute of Technology,</span> 
            <span class="author-block"><sup>2</sup>National University of Singapore,</span>
            <span class="author-block"><sup>3</sup>Stanford Unversity</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <img src="./static/images/GeorgiaTech_RGB.png" width="250" align="absmiddle"/>  
            </span>
	    <span class="author-block">
              <img src="./static/images/nus-logo.png" width="150" align="absmiddle"/>  
            </span>
	    <span class="author-block">
              <img src="./static/images/stanford-university-logo-2.png" width="200" align="absmiddle"/>  
	    </span>
          </div>
          <div class="container is-max-desktop content">
            <br>
            <h3 class="title is-3 publication-title" style="margin-bottom:0">Highlight</h3>
            <br>
            <p>Voice assistants such as Siri and Google Assistant often model speech and language separately. This loses information from speech and makes improving end-to-end systems more challenging. However, recent efforts to learn end-to-end Speech Large Language Models to address this often lose useful capabilities from text-only LLMs.</p>
            <br>
            <p>We introduce DiVa, a distilled end-to-end voice assistant model.  DiVA is trained using context distillation to learn a speech encoder without degrading existing capabilities, aiming to trigger the same response the model across modalities, rather than supervised labels. This enables the use of existing large-scale ASR corpora as a proxy for massively multi-task training data while preserving the output distribution of the original model. DiVa is then trained on the English Common Voice with over 3500 hours of natural and diverse speech contributed by a global pool of volunteers. Experimental results show that DiVa outperforms existing Speech LLMs at zero-shot spoken question answering, emotion recognition, and speech translation.</p>
	        </div>
          <iframe allow="microphone" style="height:75vh;width:80vw" src="https://06d2-34-170-119-192.ngrok-free.app" title="gradio demo"></iframe>
        </div>
          
        </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h3 class="title is-3 publication-title" style="margin-bottom:0">Motivation</h3>
    <br>
	  <p>As Large Language Models (LLMs) capabilities increase, so does the value of transferring these capabilities to new modalities, including audio and speech. Speech interaction is an especially natural interface for language technology, with measurable efficiency gains for users. The simplest method of integrating speech with LLMs is to feed audio input to an Automatic Speech Recognition (ASR) model and produce a text transcription for the LLM to use. However, this process loses socio-phonetic information such as tone, pacing, and accent regardless of the transcription accuracy. Improving these pipelined systems also requires annotations for both transcriptions and expected outputs, increasing annotation complexity and cost.</p>
    <br>
    <p>Models that can accept speech directly as input have the potential to simplify and accelerate inference, reduce annotation costs, and capture the rich social information inevitably lost by ASR. In this pursuit, a variety of works have trained audio-encoders on top of LLM. These methods all utilize the same well-established approach: large-scale multitask supervised finetuning (SFT). </p>
    <br>
    <p>However, models trained using only SFT face several challenges. First, they often struggle to generalize to speech tasks beyond those represented in the training data, even if the base LLM can perform similar tasks in response to text. As observed in prior work (Tang et al., 2023), freezing the weights of the text-only LLM is not sufficient to address this. Since SFT data for speech is currently less abundant than for text, this means the resulting models have a comparatively limited range of capabilities.</p>
    <br>
    <p>On the other hand, speech corpora focused on instruction following either include only a small number of speakers or consist only of evaluation data. While some recent speech corpora have focused on diversity and representation, their open-ended conversations and monologues are unsuitable for training an instruction-following model. Reliance on SFT data thereby exacerbates under-representation of those already the least supported by speech technologies. As such, speech LMs trained with SFT risk increasing frustration with state-of-the-art speech systems and preventing their use by individuals for whom interacting with technology through speech is most valuable.</p>
    <br>
    <p>Our main research question is how to create a unified model for speech assistance using only ASR data, which exists at large-scales both online and in community-created corpora such as the CommonVoice. In order to achieve generalizability despite training on data from only a single task, we focus on minimizing catastrophic forgetting in an early fusion model initialized from a pretrained ASR model and LLM.</p>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{held2024diva,
        author="Held, Will and Zhang, Yanzhe and Ryan, Michael and Shi, Weiyan and Li, Ella and Yang, Diyi",
        title="Distilling an End-to-End Voice Assistant from Speech Recognition Data",
        year="2024",
        publisher="HuggingFace",
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This source code of this website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
